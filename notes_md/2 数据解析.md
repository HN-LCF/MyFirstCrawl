# 二、数据解析

## 0 基础

### 概要

* 数据解析
  * 正则表达式（后scrapy框架用）
  * bs4
  * xpath
  * pyquery（自学）

## 1 正则表达式

### 1.1 正则解析

* **正则基本概念：**
  * 1、限定符
    * ？ 前面的字符出现0次或1次
    * \* ≥0次
    * \+ ≥1次
    * {m} 出现m次
    * {m,} ≥m次
    * {m,n} 出现次数在m-n之间
    * 如果希望匹配多个字符的重复，则将想要匹配的字符用()括起来，再添加限定符*
  * 2、“或”运算符
    * 竖线 |
  * 3、字符类
    * [a-w] 取的字符必须在方括号中；也可在[]中指定字符范围，如[a-z]表示所有小写英文字符，[a-zA-Z]表示所有小写大写英文字符，[a-zA-Z0-9]代表所有大小写英文字符和数字。如果在方括号前面加一个尖号^，表示匹配除尖号后面列出的字符以外的字符，如[^0-9]表示匹配非数字字符（包含换行符）
  * 4、元字符
    * \d：数字字符
  \D：非数组字符
  \w：单词字符（英文、数字、下划线）
  \W：非单词字符
  \s：空白符，包括空格、Tab制表符、换行符和换页符
  \S：非空白字符
  . ：任意字符，不包含换行符
  ^ ：匹配行首
  $ ：匹配行尾
  * 5、贪婪与懒惰匹配
    * ![正则基本概念.png](C:\Users\86199\Pictures\Screenshots\205.png)

* **用途**：
  * 使用正则进行图片数据的批量解析爬取
  * 如何爬取图片数据？
    * 方式1：基于requests
    * 方式2：基于urllib
        * urllib模块作用与requests一样，都是基于网络请求的模块
        * requests问世后迅速取代了urllib

> * **实例1：分别使用requests和urllib爬取图片**

```python
import requests

def imageCrawl_requests():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68 '
    }
    img_url = 'https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fa2.att.hudong.com%2F74%2F24' \
              '%2F23300001248577135219240898162.jpg&refer=http%3A%2F%2Fa2.att.hudong.com&app=2002&size=f9999,' \
              '10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1616133427&t=8cc5c8563e57dabed1cf2eeebe33e886 '
    response = requests.get(url=img_url, headers=headers)
    # content返回的是二进制形式的响应数据
    img_data = response.content
    with open('./1.jpg', 'wb') as fp:
        fp.write(img_data)
```

```python
import requests

def imageCrawl_urllib():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68 '
    }
    img_url = 'https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fa2.att.hudong.com%2F74%2F24' \
              '%2F23300001248577135219240898162.jpg&refer=http%3A%2F%2Fa2.att.hudong.com&app=2002&size=f9999,' \
              '10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1616133427&t=8cc5c8563e57dabed1cf2eeebe33e886 '
    # urlretrieve方法可以直接对url发起请求并进行持久化存储
    urllib.request.urlretrieve(img_url, './2.jpg')
```

- 上述两种爬取图片操作的不同之处在于：
    - 使用urllib爬取图片无法进行UA伪装，而requests的方式可以
- **需求：爬取校花网中的图片数据**
    - url:<https://nice.ruyile.com/>
    - 操作：需要将每一张图片的url解析出来， 然后对地址发起请求爬取


## 2 bs4解析

### 2.1 bs4基础

- 数据解析：
    - 数据解析的作用：
        - 实现聚焦爬虫，即用来爬取局部数据
    - 网页中显示的数据存储在哪里？
        - HTML标签以树状结构存在
        - 存储在HTML标签中或者标签的属性中
    - html
        - head
            - meta
            - title
        - body
            - div
            - div
            - div
    - 数据解析的通用原理：
        - 指定标签的定位
        - 取出标签或标签属性中存储的数据

- bs4：
    - bs4解析原理（编码流程）：
        - 实例化一个BeautifulSoup的对象，且将待解析的页面源码数据加载到该对象中
        - 调用BeautifulSoup对象中相关方法或属性进行标签定位与文本数据的提取
    - 环境安装：
        - pip install lxml #解析器
        - pip install bs4
    - BeautifulSoup对象的实例化：
        - `BeautifulSoup(fp, 'lxml')`:用来将本地寻存储的html文件数据解析
        - `BeautifulSoup(self, 'lxml')`:用来将网络请求到的页面源码数据进行解析
    - 标签定位：
        - soup.tagName:只可以定位到第一次出现tagName处
        - soup.find('tagName', attrName='value'):属性定位
        - soup.findAll():与find方法类似，但findAll方法返回列表
        - soup.select('选择器‘):选择器定位
            - 类选择器
            - ID选择器
            - 层级选择器
                - \>:表示一个层级
                - 空格:表示多个层级
  - 取数据：
      - \.text:返回该标签下所有的文件内容
      - \.string:返回该标签直系的文件内容
  - 取属性：
      - tag['attrName']

### 2.2 bs4实战

> * **实例1：解析test_bs4.html**

```python
from bs4 import BeautifulSoup

def pageParser_bs4():
    """
    使用bs4的BeautifulSoup类解析网页Html代码
    操作：
        1.实例化对象
        2.定位所需标签
        3.取文本、取属性
    :return:
    """
    fp = open('E:/程序设计/PyCharm/项目/myFirstCrawl/Test/test_bs4.html', 'r', encoding="utf-8", errors="ignore")
    soup = BeautifulSoup(fp, 'lxml')
    # soup.p
    # soup.find('li', class_='level2')
    # soup.findAll('li', class_='level2')
    # soup.select('.level2')
    pTag = soup.findAll('p')
    for tag in pTag:
        print(tag.string)
    liTag = soup.select('.tag > ul > li > span > a')
    for tag in liTag:
        print(tag['href'])
    # tag['attrName']--标签属性
    # tag.text/string--标签文本
```

> * **实例2：爬取三国演义**
    > url:<https://www.shicimingju.com/book/sanguoyanyi.html>

```python
import requests
from bs4 import BeautifulSoup

def novelCrawl():
    """
    爬取《三国演义》小说全篇
    url：https://www.shicimingju.com/book/sanguoyanyi.html
    :return:
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68 '
    }
    main_url = 'https://www.shicimingju.com/book/sanguoyanyi.html'
    fp = open('./TextLibs/sanguo.txt', 'w', encoding='utf-8')
    page = requests.get(url=main_url, headers=headers)
    page.encoding = 'utf-8'
    self = page.text
    # 数据解析：章节标题，详情页url，章节内容
    m_soup = BeautifulSoup(self, 'lxml')
    # 所有目录所在的a标签（列表）
    a_list = m_soup.select('.book-mulu > ul > li > a')
    for a in a_list:
        title = a.string
        detail_url = 'https://www.shicimingju.com/' + a['href']
        # 对详情页发起请求解析内容
        page_detail = requests.get(url=detail_url, headers=headers)
        page_detail.encoding = 'utf-8'
        page_text_detail = page_detail.text
        d_soup = BeautifulSoup(page_text_detail, 'lxml')
        chapter_tag = d_soup.find('div', class_='chapter_content')
        chapter = chapter_tag.text
        fp.write(title + ':' + chapter + '\n')
        print(title, '保存成功!!!')
    fp.close()
```

## 3 xpath解析

### 3.1 xpath基础

- xpath
    - xpath解析原理（编码流程）：
        - 实例化一个etree的对象，且将待解析的页面源码数据加载到该对象中
        - 调用etree的xpath方法并结合不同的xpath表达式实现标签的定位和数据提取
    - 环境安装：
        - pip install lxml
    - etree对象的实例化：
        - `etree.parse(fileName)`:将本地html文档加载到该对象中
        - `etree.HTML(self)`:将网站上获取的页面数据加载到该对象中
    - 标签定位
        - 最左侧的/（指定标签的绝对路径--少用）:如果xpath表达式最左侧是以/开头，则该xpath表达式一定要从根标签开始定位指定标签
        - 非最左侧的/:表示一个层级
        - 非最左侧的//:表示多个层级
        - 最左侧的//:xpath表达式可以从任意位置进行指定标签定位
             ```python
            # Locate meta:
            tree.xpath('/html/head/meta')
            tree.xpath('/html//meta')
            tree.xpath('//meta')
            ```
        - 属性定位：tagName[@attrName="value"]--`tree.xpath('//div[@class="tag"]//li'`
        - 索引定位(index自1起)：tagName[@attrName="value"]//targetName[index]--`tree.xpath('//div[@class="tag"]//li[1]'`
        - 模糊匹配：
            - `//div[contains(@class, "ng")]`
            - `//div[starts-with(@class, "ng")]`
  - 取数据
      - /text():直系文本数据
      - //text():所有文本数据
      - `tree.xpath('//a[@id="fang"]/text()')`
  - 取属性
      - /@attrName
      - `tree.xpath('//a[@id="fang"]/@href')`

### 3.2 xpath实战

> * **实例1：基础练习**

```python
from lxml import etree

def pageParse_xpath():
    """

    -----------------------------------------------------------------
    * 标签定位：
        * 路径定位--meta:
            tree.xpath('/html/head/meta')
            tree.xpath('/html//meta')
            tree.xpath('//meta')
        * 属性定位：
            tree.xpath('//div[@class="tag"]//li')
        * 引定位：
            tree.xpath('//div[@class="tag"]//li[1]')
    * 取文本：
        tree.xpath('//a[@id="fang"]/text()')
    * 取属性：
        tree.xpath('//a[@id="fang"]/@href')
    :return:
    """
    parser = etree.HTMLParser(encoding="utf-8")
    tree = etree.parse('./Test/test_parse.html', parser=parser)
    a = tree.xpath('//a[@id="fang"]/@href')
    print(a)
    print(isinstance(a, list))

```

## 4 pyquery(略)
